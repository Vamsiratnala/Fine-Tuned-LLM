{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMVr/M/VLDg+EXuzIMSYlkh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vamsiratnala/Fine-Tuned-LLM/blob/main/cleared_metadata.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWXbaE1BAF6Z"
      },
      "outputs": [],
      "source": [
        "# Install Conda in Colab\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a clean Conda environment with older versions\n",
        "!conda install -y python=3.10 numpy=1.24.3 tensorflow=2.13.0 transformers=4.38.2\n"
      ],
      "metadata": {
        "id": "7ULt1rlXAtd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import transformers\n",
        "\n",
        "print(\"âœ… Current Library Versions:\")\n",
        "print(f\"NumPy version      : {np.__version__}\")\n",
        "print(f\"TensorFlow version : {tf.__version__}\")\n",
        "print(f\"Transformers version: {transformers.__version__}\")\n"
      ],
      "metadata": {
        "id": "h-AoI78OGBXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv('/content/SMSSpamCollection.csv',sep='\\t',header = None,names=['label','message'])\n",
        "print(df.head())\n",
        "print(df.shape)\n",
        "print(df['label'].value_counts())"
      ],
      "metadata": {
        "id": "VvwT__ZTGK_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['label'].unique())\n",
        "df['label'] = df['label'].map({'ham': 0, 'spam': 1})\n",
        "print(df['label'].value_counts())\n"
      ],
      "metadata": {
        "id": "94xj1dbaGmEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#converting df to lists\n",
        "all_labels = df['label'].tolist()\n",
        "all_texts = df['message'].tolist()\n",
        "# splitting data\n",
        "train_texts,temp_texts,train_labels,temp_labels = train_test_split(all_texts,all_labels,test_size=0.3,stratify=all_labels,random_state = 42)\n",
        "val_texts,test_texts,val_labels,test_labels = train_test_split(temp_texts,temp_labels,test_size=0.5,stratify=temp_labels,random_state = 42)"
      ],
      "metadata": {
        "id": "eWP7KDh3GyEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import DistilBertTokenizerFast\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "train_encodings = tokenizer(train_texts,truncation=True,padding=True)\n",
        "val_encodings = tokenizer(val_texts,truncation=True,padding=True)\n",
        "test_encodings = tokenizer(test_texts,truncation=True,padding=True)"
      ],
      "metadata": {
        "id": "BfaZToLIG61Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define a conversion function\n",
        "\n",
        "def convert_to_tf_dataset(encodings, labels):\n",
        "  return tf.data.Dataset.from_tensor_slices(\n",
        "      ({'input_ids':encodings['input_ids'],'attention_mask':encodings['attention_mask']},labels)\n",
        "  )\n"
      ],
      "metadata": {
        "id": "J5mIErRUHCHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = convert_to_tf_dataset(train_encodings,train_labels)\n",
        "val_dataset = convert_to_tf_dataset(val_encodings,val_labels)\n",
        "test_dataset = convert_to_tf_dataset(test_encodings,test_labels)"
      ],
      "metadata": {
        "id": "opJLxd3XHFCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 8\n",
        "\n",
        "train_dataset = train_dataset.shuffle(len(train_labels)).batch(BATCH_SIZE)\n",
        "val_dataset = val_dataset.batch(BATCH_SIZE)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "N3JWkZ6pHIj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFDistilBertForSequenceClassification\n",
        "\n",
        "model = TFDistilBertForSequenceClassification.from_pretrained(\n",
        "    'distilbert-base-uncased',\n",
        "    num_labels=2  # since we're doing binary classification: spam vs ham\n",
        ")\n"
      ],
      "metadata": {
        "id": "iRz7vYljHMax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "# Your encoded labels: 0 = ham, 1 = spam\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(train_labels),\n",
        "    y=train_labels\n",
        ")\n",
        "\n",
        "class_weights_dict = {i : weight for i, weight in enumerate(class_weights)}\n",
        "print(class_weights_dict)\n"
      ],
      "metadata": {
        "id": "Wb_EpTx9HMLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n"
      ],
      "metadata": {
        "id": "1J4eR7njHS3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume class_weights_dict is already defined, like:\n",
        "# class_weights_dict = {0: 0.55, 1: 3.56}  (example)\n",
        "\n",
        "# Use per-example loss to apply class weights\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True,\n",
        "    reduction=tf.keras.losses.Reduction.NONE\n",
        ")\n",
        "\n",
        "epochs = 1\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "    epoch_loss = 0\n",
        "    batch_count = 0\n",
        "\n",
        "    for batch in train_dataset:\n",
        "        inputs, labels = batch\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            outputs = model(inputs, training=True)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            # Step 1: Get un-reduced (per-example) loss\n",
        "            per_example_loss = loss_fn(labels, logits)\n",
        "\n",
        "            # Step 2: Look up class weight for each label in the batch\n",
        "            weights = tf.gather([class_weights_dict[0], class_weights_dict[1]], labels)\n",
        "\n",
        "            weights = tf.cast(weights, dtype=tf.float32)\n",
        "\n",
        "            # Step 3: Apply weights and reduce\n",
        "            weighted_loss = tf.reduce_mean(per_example_loss * weights)\n",
        "\n",
        "        gradients = tape.gradient(weighted_loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "        epoch_loss += weighted_loss.numpy()\n",
        "        batch_count += 1\n",
        "\n",
        "    print(f\"âœ… Epoch {epoch+1} completed | Average Loss: {epoch_loss / batch_count:.4f}\")\n"
      ],
      "metadata": {
        "id": "eZ74pS0iHlIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "for batch in val_dataset:\n",
        "    inputs, labels = batch\n",
        "    outputs = model(inputs, training=False)\n",
        "    logits = outputs.logits\n",
        "    preds = tf.argmax(logits, axis=1)\n",
        "\n",
        "    all_preds.extend(preds.numpy())\n",
        "    all_labels.append(labels.numpy())\n",
        "\n",
        "# Convert to numpy arrays\n",
        "all_preds = np.array(all_preds)\n",
        "all_labels = np.array(all_labels)\n",
        "\n",
        "# Classification report\n",
        "print(\"ðŸ“Š Classification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=[\"ham\", \"spam\"]))\n",
        "\n",
        "# Confusion matrix\n",
        "print(\"ðŸ§¾ Confusion Matrix:\")\n",
        "print(confusion_matrix(all_labels, all_preds))\n"
      ],
      "metadata": {
        "id": "X31bGUibcyHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_peds =[]\n",
        "test_labels = []\n",
        "for batch in test_datset:\n",
        "  inputs , labels = batch\n",
        "  output = model(inputs,training = false) #outputs is an object of type TFSequenceClassifierOutput.\n",
        "  logits = output.logits\n",
        "  preds = tf.argmax(logits,axis = 1)\n",
        "  test_preds.extend(preds.numpy())\n",
        "    if isinstance(labels, tf.Tensor) and len(labels.shape) == 0:\n",
        "        test_labels.append(labels.numpy())\n",
        "    else:\n",
        "        test_labels.extend(labels.numpy())\n",
        "print(preds)"
      ],
      "metadata": {
        "id": "klCgwyJJluFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"distilbert-sms-spam\")\n",
        "tokenizer.save_pretrained(\"distilbert-sms-spam\")"
      ],
      "metadata": {
        "id": "AU8LfIUzqvKW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}